# -*- coding: utf-8 -*-
"""425project_sec4_21201065_Joyee.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gKKxDeG0UAIZrz9snk1JeXImWBqyvjX9
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import os
print(os.listdir("/content/drive/MyDrive/425Project/audio"))

"""# **EASY TASK**"""

import os
import numpy as np
import librosa
import warnings
from google.colab import drive

# Refresh Google Drive connection
try:
    drive.flush_and_unmount()
except ValueError:
    pass
drive.mount('/content/drive', force_remount=True)

warnings.filterwarnings('ignore')

# Configuration
BASE_PATH = '/content/drive/MyDrive/425Project'
FEATURES_PATH = os.path.join(BASE_PATH, 'features_easy.npy')
LABELS_PATH = os.path.join(BASE_PATH, 'labels_easy.npy')

# Locate audio directory
possible_paths = [
    os.path.join(BASE_PATH, 'audio', 'genres_original'),
    os.path.join(BASE_PATH, 'audio'),
    os.path.join(BASE_PATH, 'genres_original')
]

AUDIO_PATH = None
for p in possible_paths:
    if os.path.exists(p):
        try:
            if 'rock' in os.listdir(p):
                AUDIO_PATH = p
                break
        except OSError:
            continue

if not AUDIO_PATH and os.path.exists(os.path.join(BASE_PATH, 'audio')):
    AUDIO_PATH = os.path.join(BASE_PATH, 'audio')

print(f"Audio Directory: {AUDIO_PATH}")

# Feature Extraction
if os.path.exists(FEATURES_PATH) and os.path.exists(LABELS_PATH):
    print("Loading pre-computed features from drive.")
    data = np.load(FEATURES_PATH)
    labels = np.load(LABELS_PATH)
else:
    print("Starting feature extraction from audio files.")
    features = []
    labels_list = []

    try:
        genres = sorted([g for g in os.listdir(AUDIO_PATH) if os.path.isdir(os.path.join(AUDIO_PATH, g))])
    except Exception as e:
        print(f"Error reading audio directory: {e}")
        genres = []

    for genre in genres:
        genre_dir = os.path.join(AUDIO_PATH, genre)

        if not os.path.exists(genre_dir):
            continue

        print(f"Processing genre: {genre}")

        try:
            files = os.listdir(genre_dir)
        except OSError:
            continue

        for file in files:
            if file.endswith('.wav'):
                path = os.path.join(genre_dir, file)
                try:
                    y, sr = librosa.load(path, duration=30)
                    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=128)
                    features.append(np.mean(mfcc.T, axis=0))
                    labels_list.append(genre)
                except Exception:
                    continue

    data = np.array(features)
    labels = np.array(labels_list)

    if len(data) > 0:
        np.save(FEATURES_PATH, data)
        np.save(LABELS_PATH, labels)
        print("Feature extraction complete and saved.")
    else:
        print("Error: No features extracted.")

print(f"Dataset dimensions: {data.shape}")

"""MLP-based VAE (Easy Task)"""

import os
import numpy as np
import librosa
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import warnings
from google.colab import drive

# 1. Setup and Data Loading
warnings.filterwarnings('ignore')

# Refresh Drive Connection
try:
    drive.flush_and_unmount()
except ValueError:
    pass
drive.mount('/content/drive', force_remount=True)

BASE_PATH = '/content/drive/MyDrive/425Project'
FEATURES_PATH = os.path.join(BASE_PATH, 'features_easy.npy')

# Load Features
if os.path.exists(FEATURES_PATH):
    print(f"Loading features from {FEATURES_PATH}...")
    features = np.load(FEATURES_PATH)
else:
    raise FileNotFoundError("features_easy.npy not found. Please ensure feature extraction was completed.")

# 2. Data Preparation
scaler = MinMaxScaler()
# Scale features to [0, 1] for Sigmoid compatibility
X_scaled = scaler.fit_transform(features)
data_tensor = torch.FloatTensor(X_scaled)

print(f"Data loaded and scaled. Shape: {data_tensor.shape}")

# 3. Define VAE Architecture
class BasicVAE(nn.Module):
    def __init__(self, input_dim=128, latent_dim=32):
        super(BasicVAE, self).__init__()
        # Encoder
        self.fc1 = nn.Linear(input_dim, 64)
        self.fc2_mu = nn.Linear(64, latent_dim)
        self.fc2_logvar = nn.Linear(64, latent_dim)
        # Decoder
        self.fc3 = nn.Linear(latent_dim, 64)
        self.fc4 = nn.Linear(64, input_dim)

    def encode(self, x):
        h1 = F.relu(self.fc1(x))
        return self.fc2_mu(h1), self.fc2_logvar(h1)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        h3 = F.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h3))

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

# 4. Training Loop
model = BasicVAE()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

def vae_loss(recon_x, x, mu, logvar):
    BCE = F.mse_loss(recon_x, x, reduction='sum')
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return BCE + KLD

print("Starting VAE Training...")
model.train()
losses = []

for epoch in range(100):
    optimizer.zero_grad()
    recon_batch, mu, logvar = model(data_tensor)
    loss = vae_loss(recon_batch, data_tensor, mu, logvar)
    loss.backward()
    optimizer.step()
    losses.append(loss.item())

    if epoch % 10 == 0:
        print(f"Epoch {epoch} | Loss: {loss.item():.4f}")

# Visualization
plt.figure(figsize=(10, 5))
plt.plot(losses)
plt.title("VAE Training Convergence")
plt.xlabel("Epoch")
plt.ylabel("Total Loss")
plt.grid(True)
plt.show()

# Switch to evaluation mode
model.eval()
with torch.no_grad():
    mu, logvar = model.encode(data_tensor)
    latent_features = mu.numpy()

print(f"Extracted Latent Features Shape: {latent_features.shape}")

from sklearn.cluster import KMeans
from sklearn.manifold import TSNE

# 1. K-Means Clustering
kmeans_vae = KMeans(n_clusters=10, random_state=42)
vae_clusters = kmeans_vae.fit_predict(latent_features)

# 2. Reduce dimensions to 2D for visualization using t-SNE
tsne = TSNE(n_components=2, random_state=42)
latent_2d = tsne.fit_transform(latent_features)

# 3. Plot
plt.figure(figsize=(10, 8))
scatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=vae_clusters, cmap='tab10', alpha=0.6)
plt.colorbar(scatter)
plt.title("VAE Latent Space Clusters (t-SNE)")
plt.xlabel("t-SNE Dimension 1")
plt.ylabel("t-SNE Dimension 2")
plt.show()

from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, calinski_harabasz_score

# --- Baseline: PCA + K-Means ---
# Reduce original features to 32 dims using PCA (same size as VAE latent)
pca = PCA(n_components=32)
pca_features = pca.fit_transform(X_scaled)

kmeans_pca = KMeans(n_clusters=10, random_state=42)
pca_clusters = kmeans_pca.fit_predict(pca_features)

# --- Evaluation Function ---
def evaluate_clustering(data, labels, method_name):
    sil_score = silhouette_score(data, labels)
    ch_score = calinski_harabasz_score(data, labels)
    print(f"--- {method_name} Results ---")
    print(f"Silhouette Score (Higher is better): {sil_score:.4f}")
    print(f"Calinski-Harabasz Index (Higher is better): {ch_score:.4f}")
    print("-" * 30)

# Evaluate VAE
evaluate_clustering(latent_features, vae_clusters, "VAE + K-Means")

# Evaluate PCA Baseline
evaluate_clustering(pca_features, pca_clusters, "PCA + K-Means")

"""# **MEDIUM TASK**

**Extract Mel Spectrograms**
"""

import os
import librosa
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm

DATASET_PATH = "/content/drive/MyDrive/425Project/audio"
SAMPLE_RATE = 22050
DURATION = 3
SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION

def extract_spectrograms(dataset_path):
    spectrograms = []
    labels = []
    genres = sorted([d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))])

    print(f"Found {len(genres)} genres: {genres}")

    for genre_idx, genre in enumerate(genres):
        genre_path = os.path.join(dataset_path, genre)
        print(f"Processing {genre}...")

        for file in tqdm(os.listdir(genre_path)):
            if file.endswith('.wav') or file.endswith('.mp3'):
                file_path = os.path.join(genre_path, file)

                try:
                    # 1. Load Audio
                    y, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)

                    # Pad if shorter than 3 seconds
                    if len(y) < SAMPLES_PER_TRACK:
                        y = np.pad(y, (0, int(SAMPLES_PER_TRACK - len(y))), mode='constant')

                    # 2. Compute Mel Spectrogram
                    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)

                    # 3. Convert to Log-Scale (dB)
                    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)

                    # 4. Fix Width to 128 (Crop or Pad)
                    if mel_spec_db.shape[1] > 128:
                        mel_spec_db = mel_spec_db[:, :128]
                    else:
                        pad_width = 128 - mel_spec_db.shape[1]
                        mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, pad_width)), mode='constant')

                    spectrograms.append(mel_spec_db)
                    labels.append(genre_idx)

                except Exception as e:
                    print(f"Error processing {file}: {e}")

    # Convert to Numpy Arrays
    X = np.array(spectrograms)
    X = X[..., np.newaxis]
    X = X.transpose(0, 3, 1, 2)
    y = np.array(labels)

    return X, y, genres

# --- Run Extraction ---
print("Starting Spectrogram Extraction... this might take a moment.")
X_specs, y_labels, genre_names = extract_spectrograms(DATASET_PATH)

print(f"\nExtraction Complete!")
print(f" Data Shape: {X_specs.shape}")

import numpy as np

save_path_X = "/content/drive/MyDrive/425Project/spectrograms_X.npy"
save_path_y = "/content/drive/MyDrive/425Project/labels_y.npy"
save_path_genres = "/content/drive/MyDrive/425Project/genre_names.npy"

# Save the arrays to disk
np.save(save_path_X, X_specs)
np.save(save_path_y, y_labels)
np.save(save_path_genres, genre_names)

print(f"Saved spectrograms to: {save_path_X}")
print(f"Saved labels to: {save_path_y}")
print("Success!")

"""**Convolutional VAE**"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class ConvVAE(nn.Module):
    def __init__(self, latent_dim=64):
        super(ConvVAE, self).__init__()

        # --- Encoder (Compresses Image) ---
        # Input: (1, 128, 128)
        self.enc1 = nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1)
        self.enc2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)
        self.enc3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)
        self.enc4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)

        # Flatten: 256 * 8 * 8 = 16384
        self.fc_mu = nn.Linear(16384, latent_dim)
        self.fc_logvar = nn.Linear(16384, latent_dim)

        # --- Decoder (Rebuilds Image) ---
        self.fc_dec = nn.Linear(latent_dim, 16384)

        self.dec1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)
        self.dec2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)
        self.dec3 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)
        self.dec4 = nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1)

    def encode(self, x):
        h = F.relu(self.enc1(x))
        h = F.relu(self.enc2(h))
        h = F.relu(self.enc3(h))
        h = F.relu(self.enc4(h))
        h = h.view(h.size(0), -1) # Flatten
        return self.fc_mu(h), self.fc_logvar(h)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        h = self.fc_dec(z)
        h = h.view(h.size(0), 256, 8, 8) # Unflatten
        h = F.relu(self.dec1(h))
        h = F.relu(self.dec2(h))
        h = F.relu(self.dec3(h))
        return torch.sigmoid(self.dec4(h)) # Output 0-1

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

print("ConvVAE Model Defined!")

"""**Train the CVAE**"""

from torch.utils.data import DataLoader, TensorDataset

# 1. Normalize Data (Min-Max Scaling for Images)
X_min = X_specs.min()
X_max = X_specs.max()
X_norm = (X_specs - X_min) / (X_max - X_min)

# 2. Convert to Tensor
data_tensor = torch.FloatTensor(X_norm)
dataset = TensorDataset(data_tensor)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# 3. Training Setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using Device: {device}")

model = ConvVAE(latent_dim=64).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

def loss_function(recon_x, x, mu, logvar):
    BCE = F.mse_loss(recon_x, x, reduction='sum')
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return BCE + KLD

# 4. Training Loop
model.train()
epochs = 50

for epoch in range(epochs):
    train_loss = 0
    for batch in dataloader:
        img = batch[0].to(device)
        optimizer.zero_grad()
        recon_batch, mu, logvar = model(img)
        loss = loss_function(recon_batch, img, mu, logvar)
        loss.backward()
        train_loss += loss.item()
        optimizer.step()

    if epoch % 5 == 0:
        print(f"Epoch {epoch} | Loss: {train_loss / len(dataloader.dataset):.4f}")

print("Training Complete!")

"""**Visual Check**"""

model.eval()
with torch.no_grad():
    sample = data_tensor[:5].to(device)
    recon, _, _ = model(sample)

    fig, axes = plt.subplots(2, 5, figsize=(15, 6))
    for i in range(5):
        # Original
        axes[0, i].imshow(sample[i].cpu().squeeze(), cmap='magma', origin='lower')
        axes[0, i].set_title("Original")
        axes[0, i].axis('off')

        # Reconstructed
        axes[1, i].imshow(recon[i].cpu().squeeze(), cmap='magma', origin='lower')
        axes[1, i].set_title("Reconstructed")
        axes[1, i].axis('off')
    plt.show()

"""**Advanced Clustering**"""

from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.metrics import silhouette_score, davies_bouldin_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# --- 1. Extract Latent Features ---
model.eval()
latent_vectors = []
labels_true = []

with torch.no_grad():
    for i, (img, ) in enumerate(dataloader):
        img = img.to(device)
        mu, logvar = model.encode(img)
        latent_vectors.append(mu.cpu().numpy())

all_data_tensor = data_tensor.to(device)
with torch.no_grad():
    mu, _ = model.encode(all_data_tensor)
    features_cvae = mu.cpu().numpy()

print(f"Extracted CVAE Features Shape: {features_cvae.shape}")

# --- 2. Perform Clustering & Evaluation ---
def benchmark_clustering(features, method_name, labels_pred):
    try:
        sil = silhouette_score(features, labels_pred)
        db = davies_bouldin_score(features, labels_pred)
    except ValueError:
        sil, db = -1, -1

    print(f"--- {method_name} ---")
    print(f"   Silhouette Score: {sil:.4f} (Higher is better)")
    print(f"   Davies-Bouldin:   {db:.4f} (Lower is better)")
    return sil, db

print("\nClustering Results (Medium Task):")

# A. K-Means (The Classic)
kmeans = KMeans(n_clusters=10, random_state=42)
labels_kmeans = kmeans.fit_predict(features_cvae)
benchmark_clustering(features_cvae, "K-Means", labels_kmeans)

# B. Agglomerative Clustering (Hierarchy based)
agglo = AgglomerativeClustering(n_clusters=10)
labels_agglo = agglo.fit_predict(features_cvae)
benchmark_clustering(features_cvae, "Agglomerative", labels_agglo)

# C. DBSCAN (Density based - tricky tuning)
dbscan = DBSCAN(eps=3.0, min_samples=5)
labels_dbscan = dbscan.fit_predict(features_cvae)
benchmark_clustering(features_cvae, "DBSCAN", labels_dbscan)

# --- 3. Visualization (t-SNE) ---
from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, random_state=42)
latent_2d = tsne.fit_transform(features_cvae)

plt.figure(figsize=(10, 8))
# Plotting using K-Means labels as color
scatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=labels_kmeans, cmap='tab10', alpha=0.6)
plt.colorbar(scatter, label="Cluster ID")
plt.title("CVAE Latent Space (t-SNE Projection)")
plt.xlabel("Dim 1")
plt.ylabel("Dim 2")
plt.show()

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import re

# 1. Load Data
lyrics_path = "/content/drive/MyDrive/425Project/lyrics.csv"

try:
    df_lyrics = pd.read_csv(lyrics_path)
    print(f"Loaded Lyrics Dataset: {df_lyrics.shape}")

    # 2. Simple Cleaning Function
    def clean_text(text):
        if not isinstance(text, str): return ""
        text = text.lower()
        text = re.sub(r'[^a-zA-Z\s]', '', text) # Remove punctuation
        return text

    text_col = [col for col in df_lyrics.columns if 'lyric' in col.lower() or 'text' in col.lower()]
    target_col = text_col[0] if text_col else df_lyrics.columns[-1]

    print(f"Using column '{target_col}' for lyrics.")

    # Clean lyrics
    df_lyrics['clean_text'] = df_lyrics[target_col].apply(clean_text)

    # 3. Handle Size Mismatch
    if len(df_lyrics) > 1000:
        print("Lyrics dataset is larger than audio. Truncating to first 1000 rows.")
        df_lyrics = df_lyrics.iloc[:1000]
    elif len(df_lyrics) < 1000:
        print(f"Lyrics dataset is smaller ({len(df_lyrics)}). We will truncate audio to match.")

    # 4. Extract Text Features
    tfidf = TfidfVectorizer(max_features=64, stop_words='english')
    text_features = tfidf.fit_transform(df_lyrics['clean_text']).toarray()

    print(f"Text Features Shape: {text_features.shape}")

except Exception as e:
    print(f"Error loading file: {e}")

import pandas as pd
import numpy as np
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score
from sklearn.preprocessing import StandardScaler

# 1. Load Lyrics
lyrics_path = "/content/drive/MyDrive/425Project/lyrics.csv"
df_lyrics = pd.read_csv(lyrics_path)

target_genres = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']
songs_per_genre = 100

print(f"Creating a simulated dataset aligned to: {target_genres}")

# 3. Stratified Sampling
aligned_text_data = []

# Shuffle lyrics once to ensure randomness
df_shuffled = df_lyrics.sample(frac=1, random_state=42).reset_index(drop=True)

start_idx = 0
for genre in target_genres:
    # Grab the next 100 lyrics
    end_idx = start_idx + songs_per_genre
    subset = df_shuffled.iloc[start_idx:end_idx].copy()

    # assign the genre label to match the audio folder
    subset['assigned_genre'] = genre
    aligned_text_data.append(subset)

    start_idx = end_idx

# Combine into one perfectly ordered DataFrame (1000 rows)
df_aligned = pd.concat(aligned_text_data).reset_index(drop=True)

print(f"Created Aligned Dataset: {len(df_aligned)} rows")
print(f"Structure: 100 Blues -> 100 Classical -> ... -> 100 Rock")

# 4. Clean & Extract Features
def clean_text(text):
    if not isinstance(text, str): return ""
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    return text

df_aligned['clean_text'] = df_aligned['text'].apply(clean_text)

# TF-IDF (Text Features)
tfidf = TfidfVectorizer(max_features=64, stop_words='english')
text_features = tfidf.fit_transform(df_aligned['clean_text']).toarray()

print(f"Text Features Ready: {text_features.shape}")

# 5. HYBRID FUSION & CLUSTERING
try:
    # A. Scale Audio (CVAE Features from previous step)
    scaler = StandardScaler()
    audio_scaled = scaler.fit_transform(features_cvae)

    # B. Concatenate (Audio + Text)
    hybrid_features = np.hstack((audio_scaled, text_features))
    print(f"Hybrid Features Shape: {hybrid_features.shape}")

    # C. Clustering
    print("\nRunning Hybrid Clustering...")
    kmeans_hybrid = KMeans(n_clusters=10, random_state=42)
    labels_hybrid = kmeans_hybrid.fit_predict(hybrid_features)

    # D. Evaluation
    sil_hybrid = silhouette_score(hybrid_features, labels_hybrid)
    db_hybrid = davies_bouldin_score(hybrid_features, labels_hybrid)

    print("-" * 40)
    print(f"Final Hybrid Results (Simulated Alignment)")
    print("-" * 40)
    print(f"   Silhouette Score: {sil_hybrid:.4f}")
    print(f"   Davies-Bouldin:   {db_hybrid:.4f}")

    # E. Save Results for Report
    if sil_hybrid > 0.0237:
        print("Result: Adding text features improved clustering separation!")
    else:
        print("Result: Hybrid fusion score is similar/lower (expected with simulated data).")
        print("Hybrid feature representation pipeline is complete.")

except NameError:
    print("Error: 'features_cvae' is missing. Please run the CVAE extraction block first!")

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score
from sklearn.preprocessing import StandardScaler
from google.colab import drive

# 1. Mount Drive (if disconnected)
drive.mount('/content/drive')

# 2. Load Saved Spectrograms
print("Loading saved spectrograms...")
try:
    X_specs = np.load("/content/drive/MyDrive/425Project/spectrograms_X.npy")
    print(f"Loaded Audio Data: {X_specs.shape}")
except FileNotFoundError:
    print("Error: Could not find 'spectrograms_X.npy'.")
    raise

# 3. Normalize & Prepare
X_min = X_specs.min()
X_max = X_specs.max()
X_norm = (X_specs - X_min) / (X_max - X_min)
data_tensor = torch.FloatTensor(X_norm)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 4. Quick Retrain (Restores features in ~1 min)
class ConvVAE(nn.Module):
    def __init__(self, latent_dim=64):
        super(ConvVAE, self).__init__()
        self.enc1 = nn.Conv2d(1, 32, 4, 2, 1)
        self.enc2 = nn.Conv2d(32, 64, 4, 2, 1)
        self.enc3 = nn.Conv2d(64, 128, 4, 2, 1)
        self.enc4 = nn.Conv2d(128, 256, 4, 2, 1)
        self.fc_mu = nn.Linear(16384, latent_dim)
        self.fc_logvar = nn.Linear(16384, latent_dim)
        self.fc_dec = nn.Linear(latent_dim, 16384)
        self.dec1 = nn.ConvTranspose2d(256, 128, 4, 2, 1)
        self.dec2 = nn.ConvTranspose2d(128, 64, 4, 2, 1)
        self.dec3 = nn.ConvTranspose2d(64, 32, 4, 2, 1)
        self.dec4 = nn.ConvTranspose2d(32, 1, 4, 2, 1)
    def encode(self, x):
        h = F.relu(self.enc1(x))
        h = F.relu(self.enc2(h))
        h = F.relu(self.enc3(h))
        h = F.relu(self.enc4(h))
        h = h.view(h.size(0), -1)
        return self.fc_mu(h), self.fc_logvar(h)
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    def decode(self, z):
        h = self.fc_dec(z)
        h = h.view(h.size(0), 256, 8, 8)
        h = F.relu(self.dec1(h))
        h = F.relu(self.dec2(h))
        h = F.relu(self.dec3(h))
        return torch.sigmoid(self.dec4(h))
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

print("Retraining Model (Please wait ~1 min)...")
model = ConvVAE(latent_dim=64).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
dataloader = DataLoader(TensorDataset(data_tensor), batch_size=32, shuffle=True)
model.train()
for epoch in range(20):
    for batch in dataloader:
        img = batch[0].to(device)
        optimizer.zero_grad()
        recon, mu, logvar = model(img)
        loss = F.mse_loss(recon, img, reduction='sum') + (-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()))
        loss.backward()
        optimizer.step()

# Extract Audio Features
model.eval()
with torch.no_grad():
    mu, _ = model.encode(data_tensor.to(device))
    features_cvae = mu.cpu().numpy()

# 5. Hybrid Fusion (Simulated Alignment)
print("Running Hybrid Fusion...")
lyrics_path = "/content/drive/MyDrive/425Project/lyrics.csv"
df_lyrics = pd.read_csv(lyrics_path)
target_genres = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']

aligned_text_data = []
df_shuffled = df_lyrics.sample(frac=1, random_state=42).reset_index(drop=True)
start_idx = 0
for genre in target_genres:
    subset = df_shuffled.iloc[start_idx : start_idx + 100].copy()
    subset['assigned_genre'] = genre
    aligned_text_data.append(subset)
    start_idx += 100
df_aligned = pd.concat(aligned_text_data).reset_index(drop=True)

# TF-IDF
df_aligned['clean_text'] = df_aligned['text'].apply(lambda x: re.sub(r'[^a-zA-Z\s]', '', str(x).lower()))
tfidf = TfidfVectorizer(max_features=64, stop_words='english')
text_features = tfidf.fit_transform(df_aligned['clean_text']).toarray()

# Fusion
scaler = StandardScaler()
audio_scaled = scaler.fit_transform(features_cvae)
hybrid_features = np.hstack((audio_scaled, text_features))

# Final Clustering
kmeans_hybrid = KMeans(n_clusters=10, random_state=42)
labels_hybrid = kmeans_hybrid.fit_predict(hybrid_features)
sil = silhouette_score(hybrid_features, labels_hybrid)
db = davies_bouldin_score(hybrid_features, labels_hybrid)

print("\n" + "="*40)
print(f"MEDIUM TASK COMPLETE")
print("="*40)
print(f"Hybrid Score (Silhouette): {sil:.4f}")
print(f"Hybrid Score (DB Index):   {db:.4f}")

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
import numpy as np

# --- Figure 1: Reconstruction Quality ---
model.eval()
with torch.no_grad():
    indices = np.random.choice(len(data_tensor), 5, replace=False)
    samples = data_tensor[indices].to(device)
    recons, _, _ = model(samples)

    # Plot
    fig, axes = plt.subplots(2, 5, figsize=(15, 6))
    for i in range(5):
        # Original
        axes[0, i].imshow(samples[i].cpu().squeeze(), cmap='magma', origin='lower')
        axes[0, i].set_title("Original Spec")
        axes[0, i].axis('off')
        # Reconstructed
        axes[1, i].imshow(recons[i].cpu().squeeze(), cmap='magma', origin='lower')
        axes[1, i].set_title("CVAE Output")
        axes[1, i].axis('off')
    plt.suptitle("Figure 1: CVAE Reconstruction Quality")
    plt.show()

# --- Figure 2: t-SNE Comparison (Audio vs. Hybrid) ---
print("Computing t-SNE projections (this takes a moment)...")

# 1. Compute t-SNE for Audio Only
tsne = TSNE(n_components=2, random_state=42)
audio_2d = tsne.fit_transform(features_cvae)

# 2. Compute t-SNE for Hybrid
hybrid_2d = tsne.fit_transform(hybrid_features)

# 3. Plot Side-by-Side
fig, axes = plt.subplots(1, 2, figsize=(18, 8))

# Audio Plot
sns.scatterplot(x=audio_2d[:,0], y=audio_2d[:,1], hue=labels_hybrid,
                palette='tab10', alpha=0.6, ax=axes[0], legend=False)
axes[0].set_title("Audio-Only Latent Space (CVAE)")
axes[0].set_xlabel("t-SNE Dim 1")
axes[0].set_ylabel("t-SNE Dim 2")

# Hybrid Plot
sns.scatterplot(x=hybrid_2d[:,0], y=hybrid_2d[:,1], hue=labels_hybrid,
                palette='tab10', alpha=0.6, ax=axes[1], legend="full")
axes[1].set_title("Hybrid Latent Space (Audio + Lyrics)")
axes[1].set_xlabel("t-SNE Dim 1")
axes[1].set_ylabel("t-SNE Dim 2")

plt.suptitle("Figure 2: Impact of Hybrid Fusion on Cluster Separation")
plt.show()

# --- Figure 3: Performance Metrics ---
metrics = ['Silhouette Score', 'Davies-Bouldin Index']
audio_scores = [0.0237, 2.96]
hybrid_scores = [0.0517, 2.34]

x = np.arange(len(metrics))
width = 0.35

fig, ax = plt.subplots(figsize=(8, 6))
rects1 = ax.bar(x - width/2, audio_scores, width, label='Audio Only')
rects2 = ax.bar(x + width/2, hybrid_scores, width, label='Hybrid (Audio+Text)')

ax.set_ylabel('Score')
ax.set_title('Figure 3: Clustering Performance Comparison')
ax.set_xticks(x)
ax.set_xticklabels(metrics)
ax.legend()

def autolabel(rects):
    for rect in rects:
        height = rect.get_height()
        ax.annotate(f'{height:.3f}',
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3), textcoords="offset points",
                    ha='center', va='bottom')

autolabel(rects1)
autolabel(rects2)
plt.show()

"""# **HARD TASK**

**Data Loading & Preprocessing**
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
from sklearn.preprocessing import OneHotEncoder
from google.colab import drive

# 1. Mount Drive
drive.mount('/content/drive')

# 2. Load Data
try:
    X_specs = np.load("/content/drive/MyDrive/425Project/spectrograms_X.npy")
    y_labels = np.load("/content/drive/MyDrive/425Project/labels_y.npy")
except FileNotFoundError:
    print("Error: Files not found.")
    raise

# 3. Preprocessing (Normalization & Encoding)
X_min, X_max = X_specs.min(), X_specs.max()
X_norm = (X_specs - X_min) / (X_max - X_min)

encoder = OneHotEncoder(sparse_output=False)
y_onehot = encoder.fit_transform(y_labels.reshape(-1, 1))

# 4. Create DataLoader
data_tensor = torch.FloatTensor(X_norm)
label_tensor = torch.FloatTensor(y_onehot)
dataset = TensorDataset(data_tensor, label_tensor)
dataloader = DataLoader(dataset, batch_size=64, shuffle=True)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print("-" * 40)
print("DATASET READY")
print("-" * 40)
print(f"Samples: {len(dataset)}")
print(f"Shape:   {X_specs.shape[1:]}")
print(f"Range:   [{X_norm.min():.2f}, {X_norm.max():.2f}]")
print("-" * 40)

"""**Model Architecture**"""

class ConditionalVAE(nn.Module):
    def __init__(self, latent_dim=64, n_classes=10):
        super(ConditionalVAE, self).__init__()

        # Encoder
        self.enc1 = nn.Conv2d(1, 32, 4, 2, 1)
        self.enc2 = nn.Conv2d(32, 64, 4, 2, 1)
        self.enc3 = nn.Conv2d(64, 128, 4, 2, 1)
        self.enc4 = nn.Conv2d(128, 256, 4, 2, 1)

        self.fc_mu = nn.Linear(16384 + n_classes, latent_dim)
        self.fc_logvar = nn.Linear(16384 + n_classes, latent_dim)

        # Decoder
        self.fc_dec = nn.Linear(latent_dim + n_classes, 16384)
        self.dec1 = nn.ConvTranspose2d(256, 128, 4, 2, 1)
        self.dec2 = nn.ConvTranspose2d(128, 64, 4, 2, 1)
        self.dec3 = nn.ConvTranspose2d(64, 32, 4, 2, 1)
        self.dec4 = nn.ConvTranspose2d(32, 1, 4, 2, 1)

    def encode(self, x, c):
        h = F.relu(self.enc1(x))
        h = F.relu(self.enc2(h))
        h = F.relu(self.enc3(h))
        h = F.relu(self.enc4(h))
        h = h.view(h.size(0), -1)
        h = torch.cat([h, c], dim=1)
        return self.fc_mu(h), self.fc_logvar(h)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z, c):
        z_c = torch.cat([z, c], dim=1)
        h = self.fc_dec(z_c)
        h = h.view(h.size(0), 256, 8, 8)
        h = F.relu(self.dec1(h))
        h = F.relu(self.dec2(h))
        h = F.relu(self.dec3(h))
        return torch.sigmoid(self.dec4(h))

    def forward(self, x, c):
        mu, logvar = self.encode(x, c)
        z = self.reparameterize(mu, logvar)
        return self.decode(z, c), mu, logvar

"""**Hyperparameter Tuning & Training**"""

n_epochs = 100
max_beta = 0.5
decay_start = 20
decay_end = 80
learning_rate = 0.0002

cvae = ConditionalVAE(latent_dim=64).to(device)
optimizer = torch.optim.Adam(cvae.parameters(), lr=learning_rate)

print("Starting High-Quality Training...")
print(f"Config: Epochs={n_epochs}, Max Beta={max_beta} (Low Beta = Sharp Images)")
print("-" * 65)
print(f"{'Epoch':<10} | {'Current Beta':<15} | {'Loss':<15} | {'Recon Loss':<15}")
print("-" * 65)

cvae.train()
for epoch in range(n_epochs):
    total_loss = 0
    total_bce = 0

    # Annealing
    if epoch < decay_start:
        current_beta = 0.0
    elif epoch > decay_end:
        current_beta = max_beta
    else:
        progress = (epoch - decay_start) / (decay_end - decay_start)
        current_beta = max_beta * progress

    for img, label in dataloader:
        img, label = img.to(device), label.to(device)

        optimizer.zero_grad()
        recon, mu, logvar = cvae(img, label)

        # Loss Components
        BCE = F.mse_loss(recon, img, reduction='sum')
        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

        loss = BCE + (current_beta * KLD)

        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        total_bce += BCE.item()

    if epoch % 10 == 0 or epoch == n_epochs - 1:
        avg_loss = total_loss / len(dataloader.dataset)
        avg_bce = total_bce / len(dataloader.dataset)
        print(f"{epoch:<10} | {current_beta:<15.2f} | {avg_loss:<15.2f} | {avg_bce:<15.2f}")

print("-" * 65)
print("Training Complete.")

"""**Quantitative Evaluation**"""

from sklearn.metrics import silhouette_score, normalized_mutual_info_score, adjusted_rand_score, confusion_matrix
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, SpectralClustering
import pandas as pd

def purity_score(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    return np.sum(np.amax(cm, axis=0)) / np.sum(cm)

print("Calculating Metrics...")

# 1. Extract Features
cvae.eval()
with torch.no_grad():
    mu, _ = cvae.encode(data_tensor.to(device), label_tensor.to(device))
    features_cvae = mu.cpu().numpy()

# 2. Baselines
X_flat = X_norm.reshape(1000, -1)

# PCA
pca = PCA(n_components=64)
features_pca = pca.fit_transform(X_flat)
pca_labels = KMeans(n_clusters=10, random_state=42).fit_predict(features_pca)

# Spectral
spectral_labels = SpectralClustering(n_clusters=10, affinity='nearest_neighbors', random_state=42).fit_predict(features_pca)

# CVAE
cvae_labels = KMeans(n_clusters=10, random_state=42).fit_predict(features_cvae)

# 3. Results
datasets = [
    ("PCA + K-Means", features_pca, pca_labels),
    ("Spectral Clustering", features_pca, spectral_labels),
    ("Conditional VAE", features_cvae, cvae_labels)
]

print("\n" + "="*80)
print(f"{'Method':<25} | {'Sil':<10} | {'NMI':<10} | {'ARI':<10} | {'Purity':<10}")
print("="*80)

for name, feats, preds in datasets:
    sil = silhouette_score(feats, preds)
    nmi = normalized_mutual_info_score(y_labels, preds)
    ari = adjusted_rand_score(y_labels, preds)
    pur = purity_score(y_labels, preds)
    print(f"{name:<25} | {sil:<10.4f} | {nmi:<10.4f} | {ari:<10.4f} | {pur:<10.4f}")

print("="*80)

"""**Visualization**"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE

# Figure 1: Latent Space
tsne = TSNE(n_components=2, random_state=42)
latent_2d = tsne.fit_transform(features_cvae)

plt.figure(figsize=(10, 8))
sns.scatterplot(x=latent_2d[:,0], y=latent_2d[:,1], hue=y_labels, palette='tab10', alpha=0.8, s=60)
plt.title("Figure 1: Latent Space Distribution")
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Figure 2: Reconstruction Quality
cvae.eval()
with torch.no_grad():
    indices = np.random.choice(len(data_tensor), 5, replace=False)
    samples = data_tensor[indices].to(device)
    labels = label_tensor[indices].to(device)

    recons, _, _ = cvae(samples, labels)

    fig, axes = plt.subplots(2, 5, figsize=(15, 5))
    for i in range(5):
        axes[0, i].imshow(samples[i].cpu().squeeze(), cmap='magma', origin='lower')
        axes[0, i].set_title("Original")
        axes[0, i].axis('off')

        axes[1, i].imshow(recons[i].cpu().squeeze(), cmap='magma', origin='lower')
        axes[1, i].set_title("Reconstructed")
        axes[1, i].axis('off')

    plt.suptitle("Figure 2: Reconstruction Quality Check")
    plt.show()

"""**Final Baselines & Multi-modal Evaluation**"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, normalized_mutual_info_score, adjusted_rand_score, confusion_matrix
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.feature_extraction.text import TfidfVectorizer
import re

# --- 1. Train Improved Autoencoder
print("Training Improved Autoencoder (Baseline)...")

class ImprovedAE(nn.Module):
    def __init__(self):
        super(ImprovedAE, self).__init__()
        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(128*128, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Linear(512, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Linear(128, 64) # Latent dim 64
        )
        # Decoder (Needed for proper training)
        self.decoder = nn.Sequential(
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Linear(128, 512),
            nn.ReLU(),
            nn.Linear(512, 128*128),
            nn.Sigmoid()
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return encoded, decoded

# Flatten data for AE
X_flat = X_norm.reshape(1000, -1)
ae_tensor = torch.FloatTensor(X_flat).to(device)
ae_model = ImprovedAE().to(device)
ae_optimizer = torch.optim.Adam(ae_model.parameters(), lr=1e-3)

for epoch in range(50):
    ae_optimizer.zero_grad()
    latent, reconstructed = ae_model(ae_tensor)
    loss = nn.MSELoss()(reconstructed, ae_tensor)
    loss.backward()
    ae_optimizer.step()

# Extract AE Features
with torch.no_grad():
    features_ae, _ = ae_model(ae_tensor)
    features_ae = features_ae.cpu().numpy()


# --- 2. Load Lyrics for Multi-Modal ---
print("Loading Lyrics for Final Fusion...")

try:
    df_lyrics = pd.read_csv("/content/drive/MyDrive/425Project/lyrics.csv")
    df_lyrics['clean'] = df_lyrics['text'].apply(lambda x: re.sub(r'[^a-zA-Z\s]', '', str(x).lower()))
    tfidf = TfidfVectorizer(max_features=64, stop_words='english')
    features_lyrics = tfidf.fit_transform(df_lyrics['clean'].iloc[:1000]).toarray()

    # FUSION
    features_multimodal = np.hstack((features_cvae, features_lyrics))
    print("Lyrics Integrated Successfully.")
except:
    print("Lyrics file not found. Skipping Lyric Fusion.")
    features_multimodal = features_cvae


# --- 3. FINAL EVALUATION TABLE (With Crash Prevention) ---
def get_metrics(features, labels_true):
    kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)
    preds = kmeans.fit_predict(features)

    unique_labels = len(np.unique(preds))
    if unique_labels < 2:
        return [0.0, 0.0, 0.0, 0.1] # Return zeros if mode collapse occurs

    return [
        silhouette_score(features, preds),
        normalized_mutual_info_score(labels_true, preds),
        adjusted_rand_score(labels_true, preds),
        purity_score(labels_true, preds)
    ]

print("\n" + "="*85)
print(f"{'Method':<30} | {'Sil':<8} | {'NMI':<8} | {'ARI':<8} | {'Purity':<8}")
print("="*85)

experiments = [
    ("PCA + K-Means", features_pca),
    ("Spectral Clustering", features_pca),
    ("Autoencoder + K-Means", features_ae),
    ("CVAE (Audio+Genre)", features_cvae),
    ("Multi-modal (CVAE+Lyrics)", features_multimodal)
]

for name, feats in experiments:
    s, n, a, p = get_metrics(feats, y_labels)
    print(f"{name:<30} | {s:<8.4f} | {n:<8.4f} | {a:<8.4f} | {p:<8.4f}")

print("="*85)